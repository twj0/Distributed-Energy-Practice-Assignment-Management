\documentclass[12pt]{article} % ä½¿ç”¨ctexartå¤„ç†ä¸­æ–‡
\usepackage{geometry} % è®¾ç½®é¡µé¢æ ¼å¼
\geometry{a4paper, margin=1in}
\usepackage{amsmath}    % æ•°å­¦å…¬å¼
\usepackage{graphicx}   % æ’å…¥å›¾ç‰‡
\usepackage{multirow}   % è¡¨æ ¼ä¸­åˆå¹¶å¤šè¡Œ
\usepackage{setspace}   % è®¾ç½®è¡Œè·
\usepackage{caption}    % å›¾è¡¨æ ‡é¢˜
\usepackage{float}      % å›¾ç‰‡æµ®åŠ¨æ§åˆ¶, H å®šä½ç¬¦
\usepackage{booktabs}   % ä¸“ä¸šä¸‰çº¿è¡¨ 
\usepackage{verbatim}   % verbatimç¯å¢ƒ, ç”¨äºä»£ç å—
\usepackage{tabularx}   % æ›´çµæ´»çš„è¡¨æ ¼å®½åº¦æ§åˆ¶
\usepackage{longtable}  % é•¿è¡¨æ ¼ï¼Œå¯è·¨é¡µ 
\usepackage{ctex}       % å¤„ç†ä¸­æ–‡
\usepackage{amssymb}    % æ•°å­¦ç¬¦å·
\usepackage{pythonhighlight} % ä»£ç é«˜äº®
% --- å¯é€‰å®åŒ… ---
\usepackage{hyperref} % åˆ›å»ºè¶…é“¾æ¥ 
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue} % è®¾ç½®é“¾æ¥é¢œè‰²
% \usepackage{xcolor}   % ç”¨äºé¢œè‰²æ§åˆ¶ (å¦‚æœhyperrefä¸æ»¡è¶³é¢œè‰²éœ€æ±‚)

% --- æ ¼å¼è®¾ç½® ---
\setstretch{1.5}          % è®¾ç½®1.5å€è¡Œè·
\setlength{\parindent}{2em} % æ®µè½é¦–è¡Œç¼©è¿›2ä¸ªä¸­æ–‡å­—ç¬¦

% --- å°é¢æ¨¡æ¿å®šä¹‰ ---
\newcommand{\cover}[6]{
  \begin{center}
    \vspace*{2cm} 
    {\Huge \bfseries ã€Šåˆ†å¸ƒå¼èƒ½æºç³»ç»Ÿæ¦‚è®ºã€‹}\\[2em] 
    {\LARGE \bfseries å®è·µä½œä¸š}\\[3em] 
    
    \begin{tabular}{rl} 
      \bfseries ä½œä¸šé¢˜ç›®ï¼š & \underline{\makebox[10cm][s]{#1}} \\[1.5em] 
      \bfseries ä¸“~~~~ä¸šï¼š & èƒ½æºä¸åŠ¨åŠ›å·¥ç¨‹ \\[1.5em]
      \bfseries ç­~~~~çº§ï¼š & 2023çº§èƒ½æºä¸åŠ¨åŠ›å·¥ç¨‹ä¸€ç­(åŒ–èƒ½æ¨ç­) \\[1.5em]
      \bfseries å­¦ç”Ÿå§“åï¼š & å”ç®å˜‰ \\[1.5em]
      \bfseries å­¦~~~~å·ï¼š & 2023428020130 \\[1.5em]
      \bfseries æŒ‡å¯¼æ•™å¸ˆï¼š & é™¶å® å‰¯æ•™æˆ \\[1.5em]
      \bfseries å¹´~æœˆ~æ—¥ï¼š & \underline{\makebox[6cm][s]{#2}} \\ 
    \end{tabular}
    \vspace*{3cm} 
  \end{center}
}

\begin{document}
\pagestyle{empty} 

% --- å°é¢ ---
\cover{åŸºäºè¿­ä»£æ³•çš„å†·çƒ­ç”µè”ä¾›ç³»ç»Ÿç»æµæ€§ä¼˜åŒ–æ¨¡å‹æ±‚è§£å®è·µ}{\today}{}{}{}{}
\newpage


% --- è¯¾ç¨‹è®ºæ–‡ä»»åŠ¡ä¹¦ ---
% (æ ¹æ®æ‚¨çš„è¦æ±‚ï¼Œä»»åŠ¡ä¹¦éƒ¨åˆ†å¯ä»¥ç®€åŒ–æˆ–ç›´æ¥è¿›å…¥æ‘˜è¦ï¼Œæ­¤å¤„æˆ‘ä¿ç•™ä¸€ä¸ªç®€åŒ–çš„ä»»åŠ¡ä¹¦æ¡†æ¶ï¼Œæ‚¨å¯ä»¥å¡«å……æˆ–åˆ é™¤)
\section*{è¯¾ç¨‹è®ºæ–‡ä»»åŠ¡ä¹¦ }
\noindent ä¸“ä¸šï¼šèƒ½æºä¸åŠ¨åŠ›å·¥ç¨‹ \quad ç­çº§ï¼š2023çº§èƒ½æºä¸åŠ¨åŠ›å·¥ç¨‹ä¸€ç­(åŒ–èƒ½æ¨ç­) \\
å­¦ç”Ÿå§“åï¼šå”ç®å˜‰ \quad å­¦å·ï¼š2023428020130 \\
è®ºæ–‡é¢˜ç›®ï¼šåŸºäºè¿­ä»£æ³•çš„å†·çƒ­ç”µè”ä¾›ç³»ç»Ÿç»æµæ€§ä¼˜åŒ–æ¨¡å‹æ±‚è§£å®è·µ

\subsection*{è®¾è®¡ç›®çš„ã€ä¸»è¦å†…å®¹åŠè¦æ±‚}
\textbf{ç›®çš„ï¼š} å›´ç»•å†·çƒ­ç”µè”ä¾›ï¼ˆCCHPï¼‰ç³»ç»Ÿç»æµæœ€ä¼˜åŒ–è¿è¡Œé—®é¢˜ï¼Œå®è·µå¹¶æŒæ¡ä½¿ç”¨æ•°å€¼è¿­ä»£æ³•ï¼ˆæ¢¯åº¦ä¸‹é™æ³•ï¼‰æ±‚è§£æ— çº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œç†è§£ç›®æ ‡å‡½æ•°ã€çº¦æŸæ¡ä»¶åŠä¼˜åŒ–æ¨¡å‹æ±‚è§£çš„åŸºæœ¬è¿‡ç¨‹ã€‚

\textbf{ä¸»è¦å†…å®¹ï¼š}
\begin{enumerate}
    \item ç®€è¿°CCHPç³»ç»Ÿä¼˜åŒ–èƒŒæ™¯åŠæœ¬æ¬¡å®è·µçš„æ„ä¹‰ã€‚
    \item ä»‹ç»æ‰€é€‰ç”¨çš„éçº¿æ€§ç›®æ ‡å‡½æ•°åŠå…¶æ•°å­¦ç‰¹æ€§ã€‚
    \item é˜è¿°æ¢¯åº¦ä¸‹é™æ³•çš„åŸºæœ¬åŸç†ï¼ŒåŒ…æ‹¬æ¢¯åº¦è®¡ç®—å’Œæ­¥é•¿ï¼ˆå­¦ä¹ ç‡ï¼‰ç¡®å®šæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºä¸€ç»´æœç´¢çš„ç²¾ç¡®çº¿æœç´¢ç­–ç•¥ã€‚
    \item å±•ç¤ºä½¿ç”¨Pythonç¼–ç¨‹å®ç°çš„ä¼˜åŒ–ç®—æ³•æµç¨‹ã€‚
    \item é€šè¿‡æ•°å€¼å®éªŒï¼Œåˆ†æç®—æ³•åœ¨ä¸åŒç›®æ ‡å‡½æ•°ä¸‹çš„æ”¶æ•›è¿‡ç¨‹ã€æœ€ç»ˆä¼˜åŒ–ç»“æœï¼Œå¹¶è®¨è®ºè¿­ä»£å‚æ•°ï¼ˆå¦‚åˆå§‹ç‚¹ã€ç²¾åº¦ï¼‰å¯¹ç»“æœçš„å½±å“ã€‚
    \item æ€»ç»“å®è·µè¿‡ç¨‹ä¸­çš„ä¸»è¦å‘ç°å’Œä½“ä¼šã€‚
\end{enumerate}

\textbf{è¦æ±‚ï¼š}
\begin{enumerate}
    \item ç®—æ³•å®ç°æ­£ç¡®ï¼Œé€»è¾‘æ¸…æ™°ã€‚
    \item ç»“æœåˆ†æåˆç†ï¼Œå›¾è¡¨è§„èŒƒã€‚
    \item è®ºæ–‡æ’°å†™ç¬¦åˆå­¦æœ¯è§„èŒƒï¼Œè¯­è¨€ç§‘å­¦ä¸¥è°¨ã€‚
\end{enumerate}
\vspace{1cm}
\noindent æŒ‡å¯¼æ•™å¸ˆç­¾å­—ï¼š\underline{\makebox[6cm][s]{}} \hspace{2cm} æ—¥æœŸï¼š\underline{\makebox[4cm][s]{}}

\newpage


% --- æ‘˜è¦ä¸å…³é”®è¯ ---
\pagenumbering{roman}
\section*{æ‘˜ è¦}
\setstretch{1.25}
\noindent å†·çƒ­ç”µè”ä¾›ï¼ˆCCHPï¼‰ç³»ç»Ÿçš„é«˜æ•ˆç»æµè¿è¡Œæ˜¯å…¶åœ¨ç°ä»£èƒ½æºä½“ç³»ä¸­å‘æŒ¥å…³é”®ä½œç”¨çš„å‰æã€‚æœ¬å®è·µæŠ¥å‘Šèšç„¦äºCCHPç³»ç»Ÿç»æµä¼˜åŒ–ä¸­çš„æ ¸å¿ƒç¯èŠ‚â€”â€”æ— çº¦æŸéçº¿æ€§è§„åˆ’é—®é¢˜çš„æ•°å€¼è§£æ³•ã€‚æœ¬æ–‡é€‰å–äº†ä¸‰ä¸ªå…¸å‹çš„äºŒå…ƒéçº¿æ€§å‡½æ•°ä½œä¸ºä¼˜åŒ–ç ”ç©¶å¯¹è±¡ï¼Œç³»ç»Ÿé˜è¿°äº†æ¢¯åº¦ä¸‹é™æ³•çš„è¿­ä»£åŸç†ï¼ŒåŒ…æ‹¬ç›®æ ‡å‡½æ•°æ¢¯åº¦çš„è§£æè®¡ç®—ã€è´Ÿæ¢¯åº¦æœç´¢æ–¹å‘çš„ç¡®å®šï¼Œä»¥åŠé€šè¿‡ä¸€ç»´çº¿æœç´¢ï¼ˆä»¥å›æº¯æ³•ä¸ºä¸»è¦åˆ†æå¯¹è±¡ï¼‰åŠ¨æ€è°ƒæ•´æ­¥é•¿ä»¥ä¿è¯ç®—æ³•æ”¶æ•›æ€§çš„ç­–ç•¥ã€‚åŸºäºPythonè¯­è¨€å¼€å‘äº†å…·æœ‰ç”¨æˆ·äº¤äº’åŠŸèƒ½çš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ï¼Œå…è®¸ç”¨æˆ·é€‰æ‹©ç›®æ ‡å‡½æ•°ã€è®¾å®šåˆå§‹å‚æ•°åŠçº¿æœç´¢æ–¹æ³•ã€‚é€šè¿‡æ•°å€¼å®éªŒï¼Œå…·ä½“è®°å½•äº†ä»åˆå§‹ç‚¹\((0,0)\)å‡ºå‘ï¼Œé‡‡ç”¨å›æº¯çº¿æœç´¢ï¼Œä»¥æ”¶æ•›ç²¾åº¦\(\varepsilon=10^{-4}\)å¯¹ä¸‰ä¸ªå‡½æ•°è¿›è¡Œä¼˜åŒ–çš„è¯¦ç»†è¿‡ç¨‹ã€‚
å®éªŒç»“æœè¡¨æ˜ï¼šå¯¹äºå‡½æ•° \(f_1(x) = 10(x_1-1)^2 + (x_2+1)^4\)ï¼Œç®—æ³•åœ¨329æ¬¡è¿­ä»£åæ”¶æ•›ï¼Œå¾—åˆ°è§£ \((1.00003266, -0.94900345)\)ï¼Œç›®æ ‡å‡½æ•°å€¼ä¸º\(6.77 \times 10^{-6}\)ï¼Œè¯¯å·®ä¸º\(8.60 \times 10^{-5}\)ã€‚å¯¹äºå…·æœ‰æŒ‘æˆ˜æ€§çš„Rosenbrockå‡½æ•° \(f_2(x) = 100(x_1^2-x_2)^2 + (x_1-1)^2\) å’Œç±»Rosenbrockå‡½æ•° \(f_3(x) = 100(x_1^2-3x_2)^2 + (x_1-1)^2\)ï¼Œåœ¨1000æ¬¡è¿­ä»£ä¸Šé™å†…ï¼Œç®—æ³•è™½èƒ½æ˜¾è‘—é™ä½ç›®æ ‡å‡½æ•°å€¼ï¼Œä½†æœªèƒ½ä½¿è¯¯å·®ä¸¥æ ¼å°äºé¢„è®¾ç²¾åº¦ï¼Œåˆ†åˆ«å¾—åˆ°è§£\((0.9213, 0.8489)\)ï¼ˆ\(f_2 \approx 6.20 \times 10^{-3}\)ï¼‰å’Œ\((0.9335, 0.2906)\)ï¼ˆ\(f_3 \approx 4.42 \times 10^{-3}\)ï¼‰ï¼Œæ˜¾ç¤ºäº†æ­¤ç±»å‡½æ•°å¯¹ç®€å•æ¢¯åº¦ä¸‹é™æ³•æ”¶æ•›é€Ÿåº¦çš„æŒ‘æˆ˜ã€‚æœ¬å®è·µéªŒè¯äº†æ‰€å®ç°ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡å…·ä½“æ•°æ®æ­ç¤ºäº†ä¸åŒå‡½æ•°ç‰¹æ€§å¯¹ä¼˜åŒ–è¿‡ç¨‹çš„å½±å“ï¼Œä¸ºåç»­å­¦ä¹ é«˜çº§ä¼˜åŒ–ç®—æ³•åŠè§£å†³å®é™…èƒ½æºç³»ç»Ÿä¼˜åŒ–é—®é¢˜æä¾›äº†æœ‰ç›Šç»éªŒã€‚
\setstretch{1.5}

\vspace{1.5em}
\noindent \textbf{å…³é”®è¯ï¼š} å†·çƒ­ç”µè”ä¾›ï¼›æ¢¯åº¦ä¸‹é™æ³•ï¼›çº¿æœç´¢ï¼›å›æº¯æ³•ï¼›æ— çº¦æŸä¼˜åŒ–ï¼›Rosenbrockå‡½æ•°ï¼›Python
\newpage

% --- ç›®å½• ---
\tableofcontents
\addcontentsline{toc}{section}{ç›® å½•}
\newpage

% --- æ­£æ–‡ ---
\pagenumbering{arabic}
\section{å¼•è¨€}
\subsection{å†·çƒ­ç”µè”ä¾›ç³»ç»ŸåŠå…¶ä¼˜åŒ–æ¦‚è¿°}
å†·çƒ­ç”µè”ä¾›ï¼ˆCombined Cooling, Heating and Power, CCHPï¼‰ç³»ç»Ÿæ˜¯ä¸€ç§å…ˆè¿›çš„èƒ½æºåˆ©ç”¨æ–¹å¼ï¼Œå®ƒé€šè¿‡å¯¹ä¸€æ¬¡èƒ½æºï¼ˆå¦‚å¤©ç„¶æ°”ï¼‰çš„æ¢¯çº§åˆ©ç”¨ï¼Œåœ¨åŒä¸€ç³»ç»Ÿä¸­åŒæ—¶äº§ç”Ÿç”µèƒ½ã€çƒ­èƒ½å’Œå†·èƒ½ï¼Œä»¥æ»¡è¶³ç”¨æˆ·çš„å¤šç§èƒ½æºéœ€æ±‚ã€‚ç›¸å¯¹äºä¼ ç»Ÿçš„èƒ½æºåˆ†ä¾›ç³»ç»Ÿï¼ŒCCHPç³»ç»Ÿå…·æœ‰æ›´é«˜çš„èƒ½æºç»¼åˆåˆ©ç”¨æ•ˆç‡ã€æ›´ä½çš„ç¯å¢ƒæ’æ”¾å’Œæ›´å¥½çš„ä¾›èƒ½å¯é æ€§ï¼Œæ˜¯æ„å»ºåˆ†å¸ƒå¼èƒ½æºç³»ç»Ÿã€æé«˜èƒ½æ•ˆå’Œæ¨åŠ¨èƒ½æºå¯æŒç»­å‘å±•çš„é‡è¦æŠ€æœ¯é€”å¾„ã€‚

ç„¶è€Œï¼ŒCCHPç³»ç»Ÿçš„è®¾è®¡å’Œè¿è¡Œæ¶‰åŠå¤æ‚çš„èƒ½é‡è½¬æ¢è®¾å¤‡ã€å¤šå˜çš„è´Ÿè·éœ€æ±‚ä»¥åŠæ³¢åŠ¨çš„èƒ½æºä»·æ ¼ï¼Œè¿™ä½¿å¾—å…¶ç»æµæ€§ä¼˜åŒ–æˆä¸ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è¯¾é¢˜ã€‚å®ç°CCHPç³»ç»Ÿçš„ç»æµæœ€ä¼˜åŒ–è¿è¡Œï¼Œå³åœ¨æ»¡è¶³ç”¨æˆ·èƒ½æºéœ€æ±‚çš„å‰æä¸‹ï¼Œæœ€å°åŒ–ç³»ç»Ÿçš„æ€»æˆæœ¬æˆ–æœ€å¤§åŒ–ç³»ç»Ÿçš„æ€»æ”¶ç›Šï¼Œå¯¹äºæå‡ç³»ç»Ÿç«äº‰åŠ›è‡³å…³é‡è¦ã€‚è¿™é€šå¸¸æ¶‰åŠåˆ°å»ºç«‹åŒ…å«è®¾å¤‡ç‰¹æ€§ã€èƒ½æºå¹³è¡¡ã€ç»æµæŒ‡æ ‡ç­‰å› ç´ çš„æ•°å­¦æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨åˆé€‚çš„ä¼˜åŒ–ç®—æ³•è¿›è¡Œæ±‚è§£ã€‚

\subsection{æœ¬å®è·µçš„ç›®çš„ä¸æ„ä¹‰}
è™½ç„¶å®é™…çš„CCHPç³»ç»Ÿä¼˜åŒ–æ¨¡å‹å¾€å¾€æ˜¯åŒ…å«å¤æ‚çº¦æŸå’Œä¼—å¤šå˜é‡çš„å¤§è§„æ¨¡é—®é¢˜ï¼Œä½†å…¶æ ¸å¿ƒå¾€å¾€å¯ä»¥å½’ç»“ä¸ºæ±‚è§£ä¸€ä¸ªæˆ–å¤šä¸ªéçº¿æ€§è§„åˆ’ï¼ˆNLPï¼‰é—®é¢˜ã€‚æœ¬è¯¾ç¨‹å®è·µæ—¨åœ¨ä»åŸºç¡€å…¥æ‰‹ï¼Œé€šè¿‡è§£å†³å…¸å‹çš„æ— çº¦æŸéçº¿æ€§ä¼˜åŒ–é—®é¢˜ï¼Œä½¿å­¦ç”ŸæŒæ¡æ•°å€¼ä¼˜åŒ–è¿­ä»£ç®—æ³•çš„åŸºæœ¬åŸç†å’Œå®ç°æ–¹æ³•ã€‚

å…·ä½“è€Œè¨€ï¼Œæœ¬å®è·µé€‰ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼Œç»“åˆç²¾ç¡®çº¿æœç´¢ç­–ç•¥ï¼Œå¯¹ä¸‰ä¸ªå…·æœ‰ä¸åŒç‰¹æ€§çš„äºŒå…ƒéçº¿æ€§å‡½æ•°è¿›è¡Œæœ€å°åŒ–æ±‚è§£ã€‚é€šè¿‡ç¼–ç¨‹å®ç°ç®—æ³•ã€è¿›è¡Œæ•°å€¼å®éªŒå¹¶åˆ†æç»“æœï¼Œæ—¨åœ¨ï¼š
\begin{enumerate}
    \item ç†è§£æ¢¯åº¦ä½œä¸ºå‡½æ•°å˜åŒ–æœ€å¿«æ–¹å‘çš„æ„ä¹‰åŠå…¶åœ¨ä¼˜åŒ–ä¸­çš„åº”ç”¨ã€‚
    \item æŒæ¡åŸºäºå¯¼æ•°ä¿¡æ¯ç¡®å®šæœç´¢æ–¹å‘å’Œæ­¥é•¿çš„åŸºæœ¬è¿­ä»£é€»è¾‘ã€‚
    \item ä½“éªŒæ•°å€¼ä¼˜åŒ–ç®—æ³•çš„è¿­ä»£æ”¶æ•›è¿‡ç¨‹ï¼Œå¹¶åˆ†æå½±å“æ”¶æ•›æ€§çš„å› ç´ ã€‚
    \item ä¸ºåç»­å­¦ä¹ æ›´å¤æ‚çš„çº¦æŸä¼˜åŒ–ç®—æ³•å’Œè§£å†³å®é™…å·¥ç¨‹ä¼˜åŒ–é—®é¢˜å¥ å®šåŸºç¡€ã€‚
\end{enumerate}
æœ¬å®è·µæ‰€é‡‡ç”¨çš„æ±‚è§£æ€è·¯å’Œæ–¹æ³•ï¼Œè™½é’ˆå¯¹ç®€åŒ–æ¨¡å‹ï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³å¯¹äºç†è§£å’Œå¼€å‘æ›´é«˜çº§çš„CCHPç³»ç»Ÿè°ƒåº¦ä¼˜åŒ–ç­–ç•¥å…·æœ‰å€Ÿé‰´æ„ä¹‰ã€‚

\section{ä¼˜åŒ–æ¨¡å‹ä¸ç®—æ³•åŸç†}
\subsection{ç›®æ ‡å‡½æ•°}
æœ¬å®è·µé€‰å–äº†ä»¥ä¸‹ä¸‰ä¸ªäºŒå…ƒéçº¿æ€§å‡½æ•°ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæ—¨åœ¨æµ‹è¯•ä¼˜åŒ–ç®—æ³•åœ¨ä¸åŒå‡½æ•°å½¢æ€ä¸‹çš„è¡¨ç°ï¼š
\begin{enumerate}
    \item å‡½æ•°1ï¼š\(f_1(x_1, x_2) = 10(x_1-1)^2 + (x_2+1)^4\)
    \item å‡½æ•°2ï¼š\(f_2(x_1, x_2) = 100(x_1^2-x_2)^2 + (x_1-1)^2\) (Rosenbrock å‡½æ•°)
    \item å‡½æ•°3ï¼š\(f_3(x_1, x_2) = 100(x_1^2-3x_2)^2 + (x_1-1)^2\)
\end{enumerate}
è¿™äº›å‡½æ•°å‡ä¸ºæ— çº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œå³å˜é‡ \(x_1, x_2\) å¯åœ¨æ•´ä¸ªå®æ•°åŸŸ \(\mathbb{R}^2\) å†…å–å€¼ã€‚å®ƒä»¬çš„ç†è®ºæœ€ä¼˜è§£ï¼ˆæœ€å°å€¼ç‚¹å’Œæœ€å°å€¼ï¼‰æ˜¯å·²çŸ¥çš„ï¼Œä¾¿äºéªŒè¯ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚å…¶ä¸­ï¼ŒRosenbrockå‡½æ•°ä»¥å…¶ç‹­é•¿çš„æŠ›ç‰©çº¿å½¢å±±è°·è€Œé—»åï¼Œå¯¹ä¼˜åŒ–ç®—æ³•çš„æ”¶æ•›æ€§æ˜¯ä¸€ä¸ªç»å…¸è€ƒéªŒã€‚

\subsection{æ¢¯åº¦ä¸‹é™æ³•åŸç†}
æ¢¯åº¦ä¸‹é™æ³•ï¼ˆGradient Descent Methodï¼‰æ˜¯ä¸€ç§ä¸€é˜¶è¿­ä»£ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºæ±‚è§£æ— çº¦æŸä¼˜åŒ–é—®é¢˜çš„æå°å€¼ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šä»ä¸€ä¸ªåˆå§‹ç‚¹ \(x^{(k)}\) å‡ºå‘ï¼Œæ²¿ç€å½“å‰ç‚¹ç›®æ ‡å‡½æ•° \(f(x)\) çš„è´Ÿæ¢¯åº¦æ–¹å‘ \(-\nabla f(x^{(k)})\) è¿›è¡Œæœç´¢ï¼Œå› ä¸ºè´Ÿæ¢¯åº¦æ–¹å‘æ˜¯å‡½æ•°å€¼ä¸‹é™æœ€å¿«çš„æ–¹å‘ã€‚è¿­ä»£æ›´æ–°å…¬å¼å¦‚ä¸‹ï¼š
\begin{equation}
    x^{(k+1)} = x^{(k)} + a_k d^{(k)}
    \label{eq:gd_update}
\end{equation}
å…¶ä¸­ï¼š
\begin{itemize}
    \item \(x^{(k)}\) æ˜¯ç¬¬ \(k\) æ¬¡è¿­ä»£çš„è§£å‘é‡ã€‚
    \item \(d^{(k)}\) æ˜¯ç¬¬ \(k\) æ¬¡è¿­ä»£çš„æœç´¢æ–¹å‘ï¼Œå¯¹äºæ¢¯åº¦ä¸‹é™æ³•ï¼Œ\(d^{(k)} = - \nabla f(x^{(k)})\)ã€‚
    \item \(a_k > 0\) æ˜¯ç¬¬ \(k\) æ¬¡è¿­ä»£çš„æ­¥é•¿ï¼ˆæˆ–ç§°å­¦ä¹ ç‡ï¼‰ã€‚
    \item \(\nabla f(x^{(k)}) = \left[ \frac{\partial f}{\partial x_1}(x^{(k)}), \frac{\partial f}{\partial x_2}(x^{(k)}) \right]^T\) æ˜¯å‡½æ•° \(f\) åœ¨ç‚¹ \(x^{(k)}\) å¤„çš„æ¢¯åº¦å‘é‡ã€‚
\end{itemize}

\subsection{æ¢¯åº¦è®¡ç®—}
å¯¹äºæœ¬å®è·µä¸­çš„ç›®æ ‡å‡½æ•°ï¼Œæ¢¯åº¦å‘é‡å¯ä»¥è§£æè®¡ç®—å¾—åˆ°ï¼š
\begin{enumerate}
    \item å‡½æ•°1çš„æ¢¯åº¦ \(\nabla f_1(x_1, x_2)\)ï¼š
    \begin{align*}
        \frac{\partial f_1}{\partial x_1} &= 20(x_1-1) \\
        \frac{\partial f_1}{\partial x_2} &= 4(x_2+1)^3
    \end{align*}
    \item å‡½æ•°2çš„æ¢¯åº¦ \(\nabla f_2(x_1, x_2)\)ï¼š
    \begin{align*}
        \frac{\partial f_2}{\partial x_1} &= 400x_1(x_1^2-x_2) + 2(x_1-1) \\
        \frac{\partial f_2}{\partial x_2} &= -200(x_1^2-x_2)
    \end{align*}
    \item å‡½æ•°3çš„æ¢¯åº¦ \(\nabla f_3(x_1, x_2)\)ï¼š
    \begin{align*}
        \frac{\partial f_3}{\partial x_1} &= 400x_1(x_1^2-3x_2) + 2(x_1-1) \\
        \frac{\partial f_3}{\partial x_2} &= -600(x_1^2-3x_2)
    \end{align*}
\end{enumerate}

\subsection{æ­¥é•¿ \(a_k\) çš„ç¡®å®šï¼šçº¿æœç´¢ç­–ç•¥}
æ­¥é•¿ \(a_k\) çš„é€‰æ‹©å¯¹æ¢¯åº¦ä¸‹é™æ³•çš„æ”¶æ•›æ€§èƒ½è‡³å…³é‡è¦ã€‚ç²¾ç¡®çº¿æœç´¢æ—¨åœ¨æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜çš„ \(a_k\)ï¼Œä½¿å¾—ç›®æ ‡å‡½æ•°åœ¨å½“å‰æœç´¢æ–¹å‘ \(d^{(k)}\) ä¸Šçš„ä¸‹é™æœ€å¤§ï¼Œå³ï¼š
\begin{equation}
    a_k = \arg\min_{a > 0} \phi(a) \quad \text{å…¶ä¸­ } \phi(a) = f(x^{(k)} + a d^{(k)})
    \label{eq:linesearch}
\end{equation}
æ±‚è§£å¼ \eqref{eq:linesearch} çš„ä¸€ä¸ªå¸¸ç”¨æ–¹æ³•æ˜¯ä»¤å…¶ä¸€é˜¶å¯¼æ•°ç­‰äºé›¶ï¼š
\begin{equation}
    \phi'(a) = \frac{d}{da} f(x^{(k)} + a d^{(k)}) = \nabla f(x^{(k)} + a d^{(k)})^T d^{(k)} = 0
    \label{eq:phi_prime}
\end{equation}
å¯¹äºæœ¬å®è·µä¸­çš„éçº¿æ€§å‡½æ•°ï¼Œå¯é‡‡ç”¨æ•°å€¼æ–¹æ³•æ±‚è§£å¼ \eqref{eq:phi_prime}ã€‚æˆ–è€…ï¼Œä½¿ç”¨å¦‚å›æº¯æ³•ç­‰éç²¾ç¡®çº¿æœç´¢æ–¹æ³•ï¼Œå¯»æ‰¾æ»¡è¶³å……åˆ†ä¸‹é™æ¡ä»¶ï¼ˆå¦‚Armijoæ¡ä»¶ï¼‰çš„æ­¥é•¿ï¼š
\begin{equation}
    f(x^{(k)} + a_k d^{(k)}) \leq f(x^{(k)}) + c \cdot a_k \nabla f(x^{(k)})^T d^{(k)}
    \label{eq:armijo}
\end{equation}
å…¶ä¸­ \(c \in (0, 1)\) æ˜¯ä¸€ä¸ªå¸¸æ•°ã€‚æœ¬æ¬¡Pythonå®ç°ä¸­æä¾›äº†å¤šç§çº¿æœç´¢ç­–ç•¥ã€‚

\subsection{ç»ˆæ­¢æ¡ä»¶}
è¿­ä»£è¿‡ç¨‹é‡‡ç”¨çš„ç»ˆæ­¢æ¡ä»¶æ˜¯å½“å‰åä¸¤æ¬¡è¿­ä»£çš„è§£å‘é‡ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»è¶³å¤Ÿå°ï¼š
\begin{equation}
    \|x^{(k+1)} - x^{(k)}\| < \varepsilon
    \label{eq:termination}
\end{equation}
å…¶ä¸­ \(\varepsilon\) æ˜¯é¢„è®¾çš„ç²¾åº¦ã€‚åŒæ—¶è®¾ç½®æœ€å¤§è¿­ä»£æ¬¡æ•°ä½œä¸ºè¾…åŠ©ç»ˆæ­¢æ¡ä»¶ã€‚

\section{ç®—æ³•å®ç°ä¸æ•°å€¼å®éªŒ}
\subsection{Python å®ç°æ¦‚è¿°}
åŸºäºå‰è¿°ä¼˜åŒ–åŸç†ï¼Œåˆ©ç”¨PythonåŠå…¶ç§‘å­¦è®¡ç®—åº“ï¼ˆNumPy, SciPy, Matplotlibï¼‰æ„å»ºäº†ä¸€ä¸ªäº¤äº’å¼çš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ã€‚è¯¥ä¼˜åŒ–å™¨å…è®¸ç”¨æˆ·åœ¨è¿è¡Œæ—¶é€‰æ‹©ç›®æ ‡å‡½æ•°ã€è¾“å…¥åˆå§‹ç‚¹ã€è®¾å®šæ”¶æ•›ç²¾åº¦ä¸æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¹¶é€‰æ‹©çº¿æœç´¢ç­–ç•¥ï¼ˆåŒ…æ‹¬åŸºäº`fsolve`æ±‚è§£\(\phi'(a)=0\)ã€å›æº¯æ³•ã€ä»¥åŠåŸºäº`minimize\_scalar`çš„1Dä¼˜åŒ–ï¼‰ã€‚ç®—æ³•æ ¸å¿ƒåœ¨äºè¿­ä»£æ›´æ–°è§£å‘é‡ï¼Œå¹¶é€šè¿‡é€‰å®šçš„çº¿æœç´¢æ–¹æ³•åŠ¨æ€ç¡®å®šæ­¥é•¿ã€‚ä¼˜åŒ–è¿‡ç¨‹æ•°æ®è¢«è®°å½•ï¼Œç»“æŸåè¾“å‡ºæ€»ç»“æŠ¥å‘Šå¹¶è‡ªåŠ¨ç”Ÿæˆã€ä¿å­˜å¯è§†åŒ–å›¾è¡¨ï¼ˆæ”¶æ•›æ›²çº¿ã€ç­‰é«˜çº¿è·¯å¾„å›¾ï¼‰ï¼Œæ–‡ä»¶ååŒ…å«è¿è¡Œå‚æ•°å’Œæ—¶é—´æˆ³ã€‚

\subsection{å®éªŒå‚æ•°è®¾ç½®}
æœ¬æŠ¥å‘Šä¸­åˆ†æçš„æ•°å€¼å®éªŒï¼Œå‡é‡‡ç”¨ä»¥ä¸‹ç»Ÿä¸€çš„å‚æ•°é…ç½®ï¼š
\begin{itemize}
    \item \textbf{åˆå§‹ç‚¹ \(x^{(0)}\)}ï¼š\((0.0, 0.0)\)
    \item \textbf{æ”¶æ•›ç²¾åº¦ \(\varepsilon\)}ï¼š\(1.0 \times 10^{-4}\)
    \item \textbf{æœ€å¤§è¿­ä»£æ¬¡æ•°}ï¼š1000
    \item \textbf{çº¿æœç´¢æ–¹æ³•}ï¼šå›æº¯æ³• (Backtracking line search)
\end{itemize}

\subsection{å®éªŒç»“æœä¸åˆ†æ}
æ ¹æ®æä¾›çš„ä¸‰æ¬¡Pythonè„šæœ¬ï¼ˆ`cchp\_optimize\_v3.py`ï¼‰è¿è¡Œè®°å½•ï¼ˆåˆ†åˆ«é’ˆå¯¹å‡½æ•°1ã€2ã€3ï¼Œå‡é‡‡ç”¨ä¸Šè¿°é»˜è®¤å‚æ•°ï¼‰ï¼Œè·å¾—äº†å…·ä½“çš„ä¼˜åŒ–æ•°æ®ã€‚

\subsubsection{å‡½æ•°1ï¼š\(f_1(x_1, x_2) = 10(x_1-1)^2 + (x_2+1)^4\)}                                                          
ç†è®ºæœ€ä¼˜è§£ä¸º \(x^*=(1, -1)\)ï¼Œ\(f(x^*)=0\)ã€‚ä»åˆå§‹ç‚¹ \((0,0)\) å‡ºå‘ï¼Œç®—æ³•åœ¨329æ¬¡è¿­ä»£åæ»¡è¶³ç»ˆæ­¢æ¡ä»¶ã€‚æœ€ç»ˆè§£ä¸º \(x^{(329)} \approx (1.00003, -0.94900)\)ï¼Œç›®æ ‡å‡½æ•°å€¼ \(f(x^{(329)}) \approx 6.77 \times 10^{-6}\)ï¼Œæœ€ç»ˆè¯¯å·® \(\|x^{(k+1)} - x^{(k)}\| \approx 8.60 \times 10^{-5} < \varepsilon\)ï¼Œæœ€ç»ˆæ¢¯åº¦èŒƒæ•° \(\|\nabla f(x^{(329)})\| \approx 6.88 \times 10^{-4}\)ã€‚ä¼˜åŒ–è¿‡ç¨‹å¦‚å›¾ \ref{fig:f1_results_actual} æ‰€ç¤ºã€‚

\begin{figure}[H]
  \centering
  % è¯·æ›¿æ¢ä¸ºå®é™…ç”Ÿæˆçš„é’ˆå¯¹å‡½æ•°1çš„ç»„åˆå›¾
  \includegraphics[width=0.9\textwidth]{../fig/opt_F1_x0_0p0_0p0_LS_backtracking_20250524-211744.png}

  \caption{å‡½æ•°1ä¼˜åŒ–è¿‡ç¨‹ï¼ˆå›æº¯çº¿æœç´¢, \(x^{(0)}=(0,0)\), \(\varepsilon=10^{-4}\)ï¼‰}
  \label{fig:f1_results_actual}
\end{figure}

\textbf{åˆ†æï¼š} ç®—æ³•æˆåŠŸæ”¶æ•›ï¼Œè§£éå¸¸æ¥è¿‘ç†è®ºæœ€ä¼˜è§£ï¼Œç›®æ ‡å‡½æ•°å€¼è¶‹è¿‘äº0ã€‚\(x_2\) åˆ†é‡æœªèƒ½ç²¾ç¡®è¾¾åˆ°-1.0ä¸»è¦å—é™äº \(\varepsilon\) åŠè¯¥ç‚¹é™„è¿‘æ¢¯åº¦è¾ƒå°çš„ç‰¹æ€§ã€‚

\subsubsection{å‡½æ•°2ï¼š\(f_2(x_1, x_2) = 100(x_1^2-x_2)^2 + (x_1-1)^2\) (Rosenbrock)}
ç†è®ºæœ€ä¼˜è§£ \(x^*=(1, 1)\)ï¼Œ\(f(x^*)=0\)ã€‚åœ¨è¾¾åˆ°1000æ¬¡æœ€å¤§è¿­ä»£æ¬¡æ•°åï¼Œç®—æ³•ç»ˆæ­¢ã€‚æœ€ç»ˆè§£ä¸º \(x^{(1000)} \approx (0.92128, 0.84889)\)ï¼Œç›®æ ‡å‡½æ•°å€¼ \(f(x^{(1000)}) \approx 6.20 \times 10^{-3}\)ï¼Œæœ€ç»ˆè¯¯å·® \(\approx 4.25 \times 10^{-4} > \varepsilon\)ï¼Œæœ€ç»ˆæ¢¯åº¦èŒƒæ•° \(\approx 0.1088\)ã€‚ä¼˜åŒ–è¿‡ç¨‹å¦‚å›¾ \ref{fig:f2_results_actual} æ‰€ç¤ºã€‚

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{../fig/opt_F2_x0_0p0_0p0_LS_backtracking_20250524-211825.png}

  \caption{Rosenbrockå‡½æ•°ä¼˜åŒ–è¿‡ç¨‹ï¼ˆå›æº¯çº¿æœç´¢, \(x^{(0)}=(0,0)\), \(\varepsilon=10^{-4}\), MaxIter=1000ï¼‰}
  \label{fig:f2_results_actual}
\end{figure}

\textbf{åˆ†æï¼š} å¯¹äºRosenbrockå‡½æ•°ï¼Œåœ¨1000æ¬¡è¿­ä»£å†…ï¼Œæ¢¯åº¦ä¸‹é™æ³•æœªèƒ½ä½¿è¯¯å·®æ”¶æ•›åˆ° \(10^{-4}\) æ°´å¹³ã€‚è·¯å¾„å›¾ï¼ˆå¦‚å›¾ \ref{fig:f2_results_actual} ä¸­çš„ç­‰é«˜çº¿éƒ¨åˆ†ï¼‰ä¼šæ˜¾ç¤ºå…¸å‹çš„æ²¿ç‹­é•¿å±±è°·ç¼“æ…¢å‰è¿›çš„â€œä¹‹â€å­—å½¢ç‰¹ç‚¹ã€‚

\subsubsection{å‡½æ•°3ï¼š\(f_3(x_1, x_2) = 100(x_1^2-3x_2)^2 + (x_1-1)^2\)}
ç†è®ºæœ€ä¼˜è§£ \(x^*=(1, 1/3 \approx 0.3333)\)ï¼Œ\(f(x^*)=0\)ã€‚åŒæ ·åœ¨è¾¾åˆ°1000æ¬¡æœ€å¤§è¿­ä»£æ¬¡æ•°åï¼Œç®—æ³•ç»ˆæ­¢ã€‚æœ€ç»ˆè§£ä¸º \(x^{(1000)} \approx (0.93355, 0.29058)\)ï¼Œç›®æ ‡å‡½æ•°å€¼ \(f(x^{(1000)}) \approx 4.42 \times 10^{-3}\)ï¼Œæœ€ç»ˆè¯¯å·® \(\approx 2.51 \times 10^{-4} > \varepsilon\)ï¼Œæœ€ç»ˆæ¢¯åº¦èŒƒæ•° \(\approx 0.1286\)ã€‚ä¼˜åŒ–è¿‡ç¨‹å¦‚å›¾ \ref{fig:f3_results_actual} æ‰€ç¤ºã€‚

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{../fig/opt_F3_x0_0p0_0p0_LS_backtracking_20250524-211846.png}
  
  \caption{å‡½æ•°3ä¼˜åŒ–è¿‡ç¨‹ï¼ˆå›æº¯çº¿æœç´¢, \(x^{(0)}=(0,0)\), \(\varepsilon=10^{-4}\), MaxIter=1000ï¼‰}
  \label{fig:f3_results_actual}
\end{figure}

\textbf{åˆ†æï¼š} å‡½æ•°3çš„ä¼˜åŒ–è¡Œä¸ºä¸Rosenbrockå‡½æ•°ç›¸ä¼¼ï¼Œä¹Ÿè¡¨ç°å‡ºæ”¶æ•›ç¼“æ…¢çš„ç‰¹æ€§ã€‚

\subsection{å¯¹ç»“æœçš„æ€è€ƒä¸è®¨è®º}
ä»ä¸Šè¿°ä¸‰ä¸ªå®éªŒç»“æœå¯ä»¥çœ‹å‡ºï¼š
\begin{enumerate}
    \item \textbf{ç®—æ³•æœ‰æ•ˆæ€§ä¸å‡½æ•°ä¾èµ–æ€§}ï¼šæ¢¯åº¦ä¸‹é™ç®—æ³•ç»“åˆå›æº¯çº¿æœç´¢ï¼Œå¯¹ç»“æ„ç›¸å¯¹ç®€å•çš„å‡½æ•°ï¼ˆ\(f_1\)ï¼‰èƒ½æœ‰æ•ˆæ”¶æ•›ã€‚å¯¹ç—…æ€ç‰¹å¾å‡½æ•°ï¼ˆ\(f_2, f_3\)ï¼‰ï¼Œå…¶æ”¶æ•›é€Ÿåº¦æ˜¾è‘—é™ä½ã€‚
    \item \textbf{è¿­ä»£å‚æ•°çš„é‡è¦æ€§}ï¼šåˆå§‹ç‚¹ã€\(\varepsilon\) å’Œæœ€å¤§è¿­ä»£æ¬¡æ•°ç›´æ¥å½±å“ä¼˜åŒ–ç»“æœã€‚
    \item \textbf{çº¿æœç´¢ç­–ç•¥çš„è§’è‰²}ï¼šå›æº¯æ³•è¡¨ç°ç¨³å¥ã€‚å¯¹äºæ”¶æ•›å›°éš¾çš„å‡½æ•°ï¼Œå°è¯•ä¸åŒï¼ˆå¯èƒ½æ›´ç²¾ç¡®çš„ï¼‰çº¿æœç´¢æ–¹æ³•ï¼Œæœ‰æ—¶èƒ½æ”¹å–„æ”¶æ•›ï¼Œä½†å¯èƒ½å¢åŠ å•æ¬¡è¿­ä»£æˆæœ¬ã€‚
    \item \textbf{æ¢¯åº¦ä¸‹é™æ³•çš„å±€é™}ï¼šæœ¬å®è·µæ¸…æ™°å±•ç¤ºäº†æ¢¯åº¦ä¸‹é™æ³•ä½œä¸ºä¸€é˜¶æ–¹æ³•çš„å›ºæœ‰å±€é™æ€§ã€‚
    \item \textbf{ä¸CCHPç³»ç»Ÿä¼˜åŒ–çš„å…³è”}ï¼šæœ¬å®è·µæ­ç¤ºçš„ä¼˜åŒ–åŸç†å’ŒæŒ‘æˆ˜å¯¹ç†è§£å®é™…CCHPç³»ç»Ÿç»æµä¼˜åŒ–è‡³å…³é‡è¦ã€‚
\end{enumerate}

\section{ç»“è®ºä¸å±•æœ›}
æœ¬å®è·µé€šè¿‡Pythonç¼–ç¨‹å®ç°äº†åŸºäºæ¢¯åº¦ä¸‹é™æ³•å’Œå¤šç§çº¿æœç´¢ç­–ç•¥çš„æ— çº¦æŸéçº¿æ€§ä¼˜åŒ–ç®—æ³•ã€‚é€šè¿‡å¯¹ä¸‰ä¸ªå…¸å‹ç›®æ ‡å‡½æ•°çš„æ•°å€¼å®éªŒï¼ŒéªŒè¯äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è§‚å¯Ÿå’Œåˆ†æäº†å…¶æ”¶æ•›ç‰¹æ€§ã€‚ä¸»è¦ç»“è®ºå¦‚ä¸‹ï¼š
\begin{enumerate}
    \item æ¢¯åº¦ä¸‹é™æ³•ç»“åˆæœ‰æ•ˆçš„çº¿æœç´¢æœºåˆ¶ï¼Œèƒ½å¤Ÿç¨³å®šåœ°æ±‚è§£æ— çº¦æŸä¼˜åŒ–é—®é¢˜ã€‚
    \item çº¿æœç´¢ç­–ç•¥çš„é€‰æ‹©å¯¹ç®—æ³•çš„æ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆç²¾åº¦æœ‰æ˜¾è‘—å½±å“ã€‚
    \item ç›®æ ‡å‡½æ•°çš„æ‹“æ‰‘ç‰¹æ€§æ˜¯å†³å®šæ¢¯åº¦ä¸‹é™æ³•æ”¶æ•›æ•ˆç‡çš„å…³é”®å¤–éƒ¨å› ç´ ã€‚
    \item é€šè¿‡ç¼–ç¨‹å®è·µï¼ŒåŠ æ·±äº†å¯¹è¿­ä»£ä¼˜åŒ–æ€æƒ³ã€æ¢¯åº¦æ¦‚å¿µã€æ­¥é•¿é€‰æ‹©é‡è¦æ€§ä»¥åŠæ•°å€¼è®¡ç®—ä¸­ç²¾åº¦ä¸æ•ˆç‡æƒè¡¡çš„ç†è§£ã€‚
\end{enumerate}

å±•æœ›æœªæ¥ï¼Œå¯ä»¥åœ¨å½“å‰å·¥ä½œåŸºç¡€ä¸Šè¿›è¡Œæ‰©å±•ï¼šç ”ç©¶å¹¶å®ç°å¦‚å…±è½­æ¢¯åº¦æ³•ã€ç‰›é¡¿æ³•ã€æ‹Ÿç‰›é¡¿æ³•ï¼ˆBFGSã€L-BFGSï¼‰ç­‰é«˜çº§æ¢¯åº¦æ–¹æ³•ï¼›å°†ç ”ç©¶æ‰©å±•åˆ°æœ‰çº¦æŸä¼˜åŒ–é—®é¢˜ï¼›å¹¶å°†æ‰€å­¦çš„ä¼˜åŒ–ç†è®ºä¸ç®—æ³•åº”ç”¨äºæ›´å…·ä½“çš„CCHPç³»ç»Ÿç»æµè°ƒåº¦æˆ–è®¾å¤‡é€‰å‹ä¼˜åŒ–æ¨¡å‹ä¸­ã€‚é€šè¿‡è¿™äº›æ›´æ·±å…¥çš„å­¦ä¹ å’Œå®è·µï¼Œå¯ä»¥ä¸ºè§£å†³å®é™…å·¥ç¨‹ä¸­çš„å¤æ‚ä¼˜åŒ–é—®é¢˜æ‰“ä¸‹æ›´åšå®çš„åŸºç¡€ã€‚


\appendix
\section{é™„å½•ï¼šPythonä»£ç }
\begin{python}
# cchp_optimize_v3.py
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import fsolve, minimize_scalar
import os
import datetime

# --- Global Variables for User Choices (will be set in main) ---
# These are set here to allow plot_optimization_progress to access them
# if they are not passed explicitly, though passing is preferred.
SELECTED_FUNCTION_NUM_GLOBAL = 2
EPSILON_GLOBAL = 1e-4

# --- Objective Functions and Gradients ---
def f1(x):
    x1, x2 = x
    return 10 * (x1 - 1)**2 + (x2 + 1)**4

def grad_f1(x):
    x1, x2 = x
    return np.array([20 * (x1 - 1), 4 * (x2 + 1)**3])

def f2(x):
    x1, x2 = x
    return 100 * (x1**2 - x2)**2 + (x1 - 1)**2

def grad_f2(x):
    x1, x2 = x
    return np.array([400 * x1 * (x1**2 - x2) + 2 * (x1 - 1), -200 * (x1**2 - x2)])

def f3(x):
    x1, x2 = x
    return 100 * (x1**2 - 3*x2)**2 + (x1 - 1)**2

def grad_f3(x):
    x1, x2 = x
    return np.array([400 * x1 * (x1**2 - 3*x2) + 2 * (x1 - 1), -600 * (x1**2 - 3*x2)])

FUNC_MAPPING = {
    1: {"func": f1, "grad": grad_f1, "name": "f(x) = 10*(x1-1)^2 + (x2+1)^4"},
    2: {"func": f2, "grad": grad_f2, "name": "f(x) = 100*(x1^2 - x2)^2 + (x1-1)^2"}, # Rosenbrock
    3: {"func": f3, "grad": grad_f3, "name": "f(x) = 100*(x1^2 - 3*x2)^2 + (x1-1)^2"},
}

# --- Line Search Helper Functions ---
def phi(a, x_k, b_k, objective_func_phi):
    """Helper function for 1D minimization: f(x_k + a*b_k)"""
    return objective_func_phi(x_k + a * b_k)

def phi_prime(a, x_k, b_k, gradient_func_phi):
    """Derivative of phi(a) w.r.t. a: grad_f(x_k + a*b_k) . b_k"""
    x_new = x_k + a * b_k
    grad_at_x_new = gradient_func_phi(x_new)
    return np.dot(grad_at_x_new, b_k)

# --- Robust Line Search Implementations ---
def line_search_fsolve(x_k, b_k, objective_func, gradient_func):
    initial_guess_a = 0.001 
    try:
        a_optimal_roots = fsolve(lambda a_val: phi_prime(a_val, x_k, b_k, gradient_func),
                                 initial_guess_a, xtol=1e-7, maxfev=500) # More precise fsolve
        
        positive_roots = [r for r in np.atleast_1d(a_optimal_roots) if r > 1e-9] 

        if not positive_roots:
            # print(f"Debug (fsolve): No positive root. Roots: {a_optimal_roots}. Fallback.")
            return line_search_backtracking(x_k, b_k, objective_func, gradient_func, alpha_init=0.1)

        best_a = -1
        min_phi_val = phi(0, x_k, b_k, objective_func) # f(x_k)
        
        # Check roots and find the one that minimizes phi(a) the most
        candidate_as = sorted([r for r in positive_roots if r < 50.0]) # Filter large steps
        if not candidate_as: # If all positive roots were too large
             return line_search_backtracking(x_k, b_k, objective_func, gradient_func, alpha_init=0.01)

        for r_val in candidate_as:
            current_phi_val = phi(r_val, x_k, b_k, objective_func)
            if current_phi_val < min_phi_val:
                min_phi_val = current_phi_val
                best_a = r_val
        
        if best_a > 1e-9 : # A valid step was found
             return np.clip(best_a, 1e-9, 20.0) # Clip to reasonable bounds
        else:
            # print(f"Debug (fsolve): No root improved f(x). Fallback.")
            return line_search_backtracking(x_k, b_k, objective_func, gradient_func, alpha_init=0.01)

    except Exception as e:
        # print(f"Error in line_search_fsolve: {e}. Fallback.")
        return line_search_backtracking(x_k, b_k, objective_func, gradient_func, alpha_init=0.01)

def line_search_backtracking(x_k, b_k, objective_func, gradient_func, alpha_init=1.0, beta=0.5, c=1e-4):
    alpha = alpha_init
    f_k = objective_func(x_k)
    # grad_k = gradient_func(x_k) # For Armijo, use grad at x_k
    # slope = np.dot(grad_k, b_k) # b_k is -grad_k, so slope = -||grad_k||^2
    # For simplicity, if b_k is already -grad_k, then grad_k.b_k = -grad_k.grad_k
    slope = np.dot(gradient_func(x_k), b_k)


    if slope > -1e-12 : # Should be significantly negative for descent
        # print(f"Warning (backtracking): Direction b_k (slope: {slope:.2e}) not a strong descent. Using small fixed step.")
        return 1e-5 # Return a very small step

    max_backtrack_iters = 50
    for _ in range(max_backtrack_iters):
        if objective_func(x_k + alpha * b_k) <= f_k + c * alpha * slope:
            return alpha
        alpha *= beta
        if alpha < 1e-10: # Prevent infinitely small steps
            # print("Warning (backtracking): Alpha too small. Using minimal step.")
            return 1e-9 
    # print("Warning (backtracking): Max backtrack iterations reached. Using last alpha.")
    return alpha # Or a very small fixed value if max iters hit

def line_search_minimize_scalar(x_k, b_k, objective_func, gradient_func_for_fallback):
    try:
        res = minimize_scalar(lambda a_val: phi(a_val, x_k, b_k, objective_func),
                              bounds=(0, 20), method='bounded', options={'xatol': 1e-7, 'maxiter':100}) # Wider bounds, more precise
        if res.success and res.x > 1e-9:
            return res.x
        else:
            # print(f"Warning (minimize_scalar): Failed (Success: {res.success}, x: {res.x:.2e}). Fallback.")
            return line_search_backtracking(x_k, b_k, objective_func, gradient_func_for_fallback, alpha_init=0.01)
    except Exception as e:
        # print(f"Error in line_search_minimize_scalar: {e}. Fallback.")
        return line_search_backtracking(x_k, b_k, objective_func, gradient_func_for_fallback, alpha_init=0.01)

# --- Gradient Descent Algorithm ---
def gradient_descent(selected_func_num, x_initial_coords, epsilon_val, max_iters_val, line_search_method_str):
    objective_func = FUNC_MAPPING[selected_func_num]["func"]
    gradient_func = FUNC_MAPPING[selected_func_num]["grad"]
    func_name_str = FUNC_MAPPING[selected_func_num]["name"]

    x_current = np.array(x_initial_coords, dtype=float)
    history = {
        'x_values': [x_current.copy()], 'f_values': [objective_func(x_current)],
        'errors': [], 'gradients_norm': [], 'step_lengths': []
    }
    iterations_completed = 0

    print(f"\nğŸš€ Optimizing: {func_name_str}")
    print(f"Initial: x0 = {x_current}, f(x0) = {history['f_values'][0]:.6e}")
    print(f"Settings: Îµ = {epsilon_val:.1e}, MaxIters = {max_iters_val}, LineSearch = {line_search_method_str}")
    print("-" * 95)
    header = f"{'Iter':<5} | {'x1':<13} | {'x2':<13} | {'f(x)':<16} | {'||âˆ‡f(x)||':<13} | {'Error':<13} | {'Step (a)':<10}"
    print(header)
    print("-" * 95)

    for k_iter in range(max_iters_val):
        iterations_completed = k_iter + 1
        grad = gradient_func(x_current)
        grad_norm = np.linalg.norm(grad)
        history['gradients_norm'].append(grad_norm)

        b_k = -grad 

        if grad_norm < epsilon_val * 0.001: # More aggressive stop if gradient is tiny
            print(f"\nâœ… Gradient norm ({grad_norm:.2e}) is very small. Optimization likely converged.")
            break
        
        if line_search_method_str == 'fsolve':
            a = line_search_fsolve(x_current, b_k, objective_func, gradient_func)
        elif line_search_method_str == 'minimize_scalar':
            a = line_search_minimize_scalar(x_current, b_k, objective_func, gradient_func)
        else: # Default to backtracking
            a = line_search_backtracking(x_current, b_k, objective_func, gradient_func)
        
        history['step_lengths'].append(a)

        if a < 1e-10: # If step length is effectively zero
            print("\nâš ï¸ Step length 'a' is extremely small. Stopping to prevent stagnation.")
            iterations_completed -=1 
            break

        x_next = x_current + a * b_k
        error = np.linalg.norm(x_next - x_current)

        history['x_values'].append(x_next.copy())
        history['f_values'].append(objective_func(x_next))
        history['errors'].append(error)
        
        # Print progress: first 15, then every 20% of max_iters, or if converged
        if k_iter < 15 or (k_iter + 1) % (max_iters_val // 20 if max_iters_val > 40 else 1) == 0 or error < epsilon_val:
            print(f"{iterations_completed:<5} | {x_next[0]:<13.6f} | {x_next[1]:<13.6f} | {history['f_values'][-1]:<16.6e} | {grad_norm:<13.2e} | {error:<13.6e} | {a:<10.4e}")

        if error < epsilon_val:
            print(f"\nğŸ Convergence achieved at iteration {iterations_completed}: Error < Epsilon.")
            break
        
        x_current = x_next

    print("-" * 95)
    if iterations_completed == max_iters_val and (not history['errors'] or history['errors'][-1] >= epsilon_val):
        print(f"\nâš ï¸ Maximum iterations ({max_iters_val}) reached. Full convergence may not be achieved.")
    
    final_x = history['x_values'][-1]
    final_f = history['f_values'][-1]
    
    print("\n--- Optimization Summary ---")
    print(f"Function Optimized: {selected_func_num} ({func_name_str})")
    print(f"Initial Point x0: {x_initial_coords}")
    print(f"Epsilon (Accuracy): {epsilon_val:.1e}")
    print(f"Line Search Method: {line_search_method_str}")
    print("-----------------------------")
    print(f"Total Iterations: {iterations_completed}")
    print(f"Optimal x = ({final_x[0]:.8f}, {final_x[1]:.8f})")
    print(f"Optimal f(x) = {final_f:.8e}")
    if history['errors']:
        print(f"Final Error (||Î”x||) = {history['errors'][-1]:.8e}")
    if history['gradients_norm']:
         print(f"Final Gradient Norm ||âˆ‡f(x)|| = {history['gradients_norm'][-1]:.8e}")

    return history

# --- Plotting and Saving ---
def plot_and_save_results(history, selected_func_num, x_initial_coords, line_search_method_str, epsilon_plot):
    objective_func_plot = FUNC_MAPPING[selected_func_num]["func"]
    func_name_str_plot = FUNC_MAPPING[selected_func_num]["name"]

    if not history['errors'] and len(history['f_values']) <= 1:
        print("\nğŸ“‰ Not enough data for detailed plotting.")
        return

    num_main_plots = 2 # f(x) vs iter, error vs iter
    x_coords = np.array(history['x_values'])
    plot_contour = x_coords.shape[0] > 1 # Only plot contour if we have iterations
    
    if plot_contour:
        num_main_plots = 3
        
    fig = plt.figure(figsize=(7 * num_main_plots, 5.5)) 

    # Plot 1: Objective Function Value
    ax1 = fig.add_subplot(1, num_main_plots, 1)
    ax1.plot(range(len(history['f_values'])), history['f_values'], marker='.', linestyle='-', color='dodgerblue')
    ax1.set_xlabel("Iteration")
    ax1.set_ylabel("f(x) value")
    ax1.set_title("Objective Function Convergence")
    if any(f_val > 0 for f_val in history['f_values']):
        try:
            ax1.set_yscale('log')
        except ValueError: # Handle cases where values are too small or non-positive for log
            ax1.set_yscale('linear')
    ax1.grid(True, which="both", ls=":", alpha=0.7)

    # Plot 2: Error
    ax2 = fig.add_subplot(1, num_main_plots, 2)
    if history['errors']:
        ax2.plot(range(1, len(history['errors']) + 1), history['errors'], marker='.', linestyle='-', color='orangered')
        ax2.axhline(y=epsilon_plot, color='k', linestyle='--', label=f'Îµ = {epsilon_plot:.1e}')
        ax2.set_ylabel("Error ||x_k+1 - x_k||")
        ax2.set_title("Error Convergence")
        ax2.set_yscale('log')
        ax2.legend()
    else:
        ax2.text(0.5, 0.5, "No error data", ha='center', va='center')
        ax2.set_title("Error Convergence")
    ax2.set_xlabel("Iteration")
    ax2.grid(True, which="both", ls=":", alpha=0.7)
    
    # Plot 3: Contour plot
    if plot_contour:
        ax3 = fig.add_subplot(1, num_main_plots, 3)
        x1_path = x_coords[:, 0]
        x2_path = x_coords[:, 1]
        
        # Auto-range for contour plot
        x1_min, x1_max = x1_path.min(), x1_path.max()
        x2_min, x2_max = x2_path.min(), x2_path.max()
        x1_range_delta = abs(x1_max - x1_min)
        x2_range_delta = abs(x2_max - x2_min)

        margin_x = max(x1_range_delta * 0.2, 0.5) # Ensure some minimum margin
        margin_y = max(x2_range_delta * 0.2, 0.5)
        
        grid_x1 = np.linspace(x1_min - margin_x, x1_max + margin_x, 100)
        grid_x2 = np.linspace(x2_min - margin_y, x2_max + margin_y, 100)
        
        if np.allclose(grid_x1, grid_x1[0]) or np.allclose(grid_x2, grid_x2[0]):
             ax3.text(0.5,0.5, "Path too small for contour.", ha='center', va='center')
        else:
            X1_grid, X2_grid = np.meshgrid(grid_x1, grid_x2)
            Z_grid = objective_func_plot(np.array([X1_grid, X2_grid]))
            
            levels = 25
            try: # Log levels for functions with large value ranges
                f_vals_positive = [f for f in history['f_values'] if f > 1e-9]
                if f_vals_positive and max(f_vals_positive) / min(f_vals_positive) > 100:
                    levels = np.logspace(np.log10(min(f_vals_positive)), np.log10(max(f_vals_positive)), 20)
            except: pass # Use linear levels if log fails
                
            cp = ax3.contour(X1_grid, X2_grid, Z_grid, levels=levels, cmap='viridis', alpha=0.7)
            fig.colorbar(cp, ax=ax3, label="f(x)", shrink=0.8)
            
            ax3.plot(x1_path, x2_path, 'o-', color='red', markersize=2, linewidth=1, label='Path')
            ax3.plot(x_initial_coords[0], x_initial_coords[1], 'X', color='blue', markersize=8, label='Start')
            ax3.plot(x1_path[-1], x2_path[-1], '*', color='lime', markersize=10, markeredgecolor='black',label='End')
            ax3.set_xlabel("x1")
            ax3.set_ylabel("x2")
            ax3.set_title("Optimization Path")
            ax3.legend(fontsize='small')
            ax3.axis('tight')
            ax3.grid(True, ls=":", alpha=0.5)

    suptitle_text = (f"Optimization: {func_name_str_plot}\n"
                     f"Initial: ({x_initial_coords[0]:.2f}, {x_initial_coords[1]:.2f}), "
                     f"LS: {line_search_method_str}, Îµ: {epsilon_plot:.1e}")
    fig.suptitle(suptitle_text, fontsize=14)
    plt.tight_layout(rect=[0, 0.02, 1, 0.93]) 

    # Saving the figure
    fig_dir = "fig"
    if not os.path.exists(fig_dir):
        os.makedirs(fig_dir)
        print(f"âœ… Created directory: ./{fig_dir}")

    time_stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    x0_str_fn = f"{str(x_initial_coords[0]).replace('.', 'p')}_{str(x_initial_coords[1]).replace('.', 'p')}"
    file_name = f"opt_F{selected_func_num}_x0_{x0_str_fn}_LS_{line_search_method_str}_{time_stamp}.png"
    file_path = os.path.join(fig_dir, file_name)

    try:
        plt.savefig(file_path, dpi=150)
        print(f"ğŸ“Š Figure saved to: {os.path.abspath(file_path)}")
    except Exception as e:
        print(f"âŒ Error saving figure: {e}")
    
    plt.show()

# --- Main Execution with User Input ---
if __name__ == "__main__":
    print("--- Gradient Descent Optimization (v3) ---")
    
    while True:
        try:
            prompt = "ğŸ¯ Select objective function (1, 2, or 3, default 2):\n"
            for i, data in FUNC_MAPPING.items():
                prompt += f"   {i}: {data['name']}\n"
            prompt += "Choice: "
            SELECTED_FUNCTION_NUM_GLOBAL = int(input(prompt) or "2")
            if SELECTED_FUNCTION_NUM_GLOBAL in FUNC_MAPPING:
                break
            print("Invalid selection. Please enter 1, 2, or 3.")
        except ValueError: print("Invalid input. Please enter a number.")

    while True:
        try:
            x0_input_str = input("ğŸ“ Enter initial point x0 as 'x1,x2' (e.g., 0,0 or -1.2,1, default 0,0): ") or "0,0"
            user_x_initial = [float(x.strip()) for x in x0_input_str.split(',')]
            if len(user_x_initial) == 2: break
            print("Please enter two comma-separated values for x1 and x2.")
        except ValueError: print("Invalid format. Use 'x1,x2' (e.g., 0.0,0.0).")
    
    while True:
        try:
            EPSILON_GLOBAL = float(input(f"ğŸ” Enter convergence epsilon Îµ (e.g., 1e-4, default {1e-4:.1e}): ") or f"{1e-4:.1e}")
            if EPSILON_GLOBAL > 0: break
            print("Epsilon must be positive.")
        except ValueError: print("Invalid number for epsilon.")

    while True:
        try:
            user_max_iterations = int(input(f"ğŸ”„ Enter maximum iterations (e.g., 1000, default 1000): ") or "1000")
            if user_max_iterations > 0: break
            print("Maximum iterations must be positive.")
        except ValueError: print("Invalid integer for maximum iterations.")

    while True:
        user_line_search_method = input(
            "ğŸŒŠ Select line search: 'fsolve', 'backtracking', 'minimize_scalar' (default 'backtracking'): "
        ).lower() or "backtracking"
        if user_line_search_method in ['fsolve', 'backtracking', 'minimize_scalar']: break
        print("Invalid method. Choose from the list.")

    # Call the optimization
    hist = gradient_descent(SELECTED_FUNCTION_NUM_GLOBAL, user_x_initial, EPSILON_GLOBAL, 
                              user_max_iterations, user_line_search_method)
    
    # Plot and save results
    plot_and_save_results(hist, SELECTED_FUNCTION_NUM_GLOBAL, user_x_initial, 
                          user_line_search_method, EPSILON_GLOBAL)

    print("\n--- Expected Optimal Solutions (Approximate) ---")
    expected = {
        1: "x â‰ˆ (1.0, -1.0), f(x) â‰ˆ 0",
        2: "x â‰ˆ (1.0, 1.0), f(x) â‰ˆ 0 (Rosenbrock)",
        3: "x â‰ˆ (1.0, 0.333), f(x) â‰ˆ 0"
    }
    print(f"For Function {SELECTED_FUNCTION_NUM_GLOBAL}: {expected.get(SELECTED_FUNCTION_NUM_GLOBAL, 'N/A')}")
    print("\nâœ¨ Optimization complete.")
\end{python}

\end{document}